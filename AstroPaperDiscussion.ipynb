{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189d87b5-808d-445d-961a-cd96f665cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import litellm\n",
    "import base64\n",
    "import json\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Suppress specific Pydantic warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
    "\n",
    "custom_api_base = \"https://litellmproxy.osu-ai.org\" \n",
    "astro1221_key = os.getenv(\"ASTRO1221_API_KEY\")\n",
    "\n",
    "def prompt_llm(messages, model=\"openai/GPT-4.1-mini\", temperature=0.2, max_tokens=1000, tools=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Send a prompt or conversation to an LLM using LiteLLM and return the response.\n",
    "    Exact function from Class 9 notebook.\n",
    "    \"\"\"\n",
    "    if isinstance(messages, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    if not (isinstance(temperature, (int, float)) and 0 <= temperature <= 2):\n",
    "        raise ValueError(\"temperature must be a float between 0 and 2 (inclusive).\")\n",
    "    if not (isinstance(max_tokens, int) and max_tokens > 0):\n",
    "        raise ValueError(\"max_tokens must be a positive integer.\")\n",
    "\n",
    "    try: \n",
    "        print(\"Contacting LLM via University Server...\")\n",
    "        response = litellm.completion(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            api_base=custom_api_base,\n",
    "            api_key=astro1221_key,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        answer = response['choices'][0]['message']['content'] #choices? what does this do, ask in class\n",
    "        if verbose: \n",
    "            print(f\"\\nSUCCESS! Here is the discussion from {model}:\\n\")\n",
    "            print(answer)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: Could not connect. Details:\\n{e}\")    \n",
    "        response = None\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529b1d9-761a-4206-9cb3-5f8d3029a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_memory(conversation_prompt):\n",
    "    for i in characters:\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": conversation_prompt})\n",
    "\n",
    "    response = prompt_llm(conversation_history)\n",
    "\n",
    "    if response is None:\n",
    "        return None\n",
    "    ai_answer = response.choices[0].message.content\n",
    "\n",
    "    # 5. CRITICAL: Add the AI's own answer to memory so it remembers what it said\n",
    "    chat_memory.append({\"role\": \"assistant\", \"content\": ai_answer})\n",
    "\n",
    "    return ai_answer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c986c-13c1-4b4a-beab-1eaa31a6b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_prompt = \"Discuss this paper.\" \n",
    "conversation_history = [{\"role\": \"user\", \"content\": conversation_prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a886a-bc96-4c33-a63f-514a8e9e4f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae273e-cca4-4946-953d-aa83afe0d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_daniel_beer = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Dr. Daniel Beer. \n",
    "    You study cosmology, including dark matter and dark energy, galactic evolution and quasars, and the intergalactic medium (IGM). \n",
    "    You are enthusiastic and inquisitive about astrophysics. \n",
    "    You are highly intelligent. \n",
    "    You will debate a variety of the ideas presented in papers.\"}\n",
    "    ] \n",
    "\n",
    "dr_davis_milk = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Dr. Davis Milk. \n",
    "     You study supermassive black holes. \n",
    "     You are curious and passionate about astrophysics. \n",
    "     You are highly intelligent.\"}\n",
    "     ]\n",
    "\n",
    "dr_phil_marg = [\n",
    "     {\"role\": \"system\", \"content\": \"You are Dr. Phil Marg. \n",
    "     You study cosmology, specifically galactic evolution and active galactic nuclei (AGN).\n",
    "     You are speculative and excited about astrophysics.\n",
    "     You are highly intelligent.\"}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b0825-d1b1-4b10-b6aa-8e8b3b8e9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [dr_daniel_beer, dr_davis_milk, dr_phil_marg] \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
